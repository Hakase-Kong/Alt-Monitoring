
# -*- coding: utf-8 -*-
import os, re, json, time, html, difflib
import datetime as dt
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

import streamlit as st
import pandas as pd
import requests

st.set_page_config(page_title="Alternative Investment Monitoring", layout="wide")

with open("config.json", "r", encoding="utf-8") as f:
    CFG = json.load(f)

EXCLUDE_TITLE_KEYWORDS = CFG["EXCLUDE_TITLE_KEYWORDS"]
ALLOWED_SOURCES = set(CFG["ALLOWED_SOURCES"])
FAVORITES = CFG["favorite_categories"]
SECTOR_FILTERS = CFG.get("sector_filter_categories", {})
COMMON_FILTERS = CFG["common_filter_categories"]
SYNONYM_MAP = CFG.get("synonym_map", {})

def init_state():
    defaults = {
        "start_date": dt.date.today() - dt.timedelta(days=7),
        "end_date": dt.date.today(),
        "keyword_input": "",
        "cat_multi": [],
        "search_results": {},
        "show_limit": {},
        "remove_duplicate_articles": True,
        "require_exact_keyword_in_title_or_content": True,
        "filter_allowed_sources_only": False,
        "use_sector_filter": True,
        "sector_filter_map": {},
        "show_sentiment_badge": False,
        "enable_summary": True,
    }
    for k,v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v
init_state()

col_title, col_option1, col_option2 = st.columns([0.6, 0.2, 0.2])
with col_title:
    st.markdown(
        "<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>ğŸ’¼ Alternative Investment Monitoring</h1>",
        unsafe_allow_html=True
    )
with col_option1:
    st.checkbox("ê°ì„±ë¶„ì„ ë°°ì§€í‘œì‹œ", key="show_sentiment_badge")
with col_option2:
    st.checkbox("ìš”ì•½ ê¸°ëŠ¥", key="enable_summary")

st.markdown("""
<style>
[data-testid='column'] > div { gap: 0rem !important; }
.stBox { background: #fcfcfc; border-radius: 0.7em; border: 1.5px solid #e0e2e6; margin-bottom: 1.2em; padding: 1.1em; box-shadow: 0 2px 8px rgba(0,0,0,0.03); }
.news-title { word-break: break-all !important; white-space: normal !important; display:block; overflow:visible; }
</style>
""", unsafe_allow_html=True)

col_kw_input, col_kw_btn = st.columns([0.8, 0.2])
with col_kw_input:
    st.text_input(label="", value="", key="keyword_input", placeholder="ì‰¼í‘œ(,)ë¡œ ë‹¤ì¤‘ í‚¤ì›Œë“œ ì…ë ¥", label_visibility="collapsed")
with col_kw_btn:
    kw_btn = st.button("ê²€ìƒ‰", use_container_width=True)

st.markdown("**â­ ì„¹í„° ì„ íƒ**")
col_cat_input, col_cat_btn = st.columns([0.8, 0.2])
with col_cat_input:
    selected_categories = st.multiselect(
        label="",
        options=list(FAVORITES.keys()),
        key="cat_multi",
        label_visibility="collapsed"
    )
with col_cat_btn:
    cat_btn = st.button("ğŸ” ê²€ìƒ‰", use_container_width=True)

date_col1, date_col2 = st.columns([1,1])
with date_col1:
    st.date_input("ì‹œì‘ì¼", key="start_date")
with date_col2:
    st.date_input("ì¢…ë£Œì¼", key="end_date")

with st.expander("ğŸ§© ê³µí†µ í•„í„° ì˜µì…˜ (í•­ìƒ ì ìš©ë¨)"):
    for major, subs in COMMON_FILTERS.items():
        st.markdown(f"**{major}**: {', '.join(subs)}" )

with st.expander("ğŸ“Š ì„¹í„°ë³„ í•„í„° ì˜µì…˜ (ëŒ€ë¶„ë¥˜ë³„ ì„¸ë¶€ ì´ìŠˆ í•„í„°ë§)"):
    st.checkbox("ì´ í•„í„° ì ìš©", key="use_sector_filter")
    updated_map = {}
    for sector in selected_categories:
        options = SECTOR_FILTERS.get(sector, [])
        default_selected = st.session_state.get("sector_filter_map", {}).get(sector, options)
        selected_sub = st.multiselect(
            f"{sector} ì„¸ë¶€ ì´ìŠˆ í‚¤ì›Œë“œ",
            options=options,
            default=default_selected,
            key=f"subfilter_sector_{sector}"
        )
        updated_map[sector] = selected_sub
    st.session_state["sector_filter_map"] = updated_map

with st.expander("ğŸ” í‚¤ì›Œë“œ í•„í„° ì˜µì…˜"):
    st.checkbox("í‚¤ì›Œë“œê°€ ì œëª© ë˜ëŠ” ë³¸ë¬¸ì— í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë³´ê¸°", key="require_exact_keyword_in_title_or_content")
    st.checkbox("ì¤‘ë³µ ê¸°ì‚¬ ì œê±°", key="remove_duplicate_articles")
    st.checkbox("íŠ¹ì • ì–¸ë¡ ì‚¬ë§Œ ê²€ìƒ‰", key="filter_allowed_sources_only", help="ALLOWED_SOURCES ëª©ë¡ë§Œ í—ˆìš©")


def expand_keywords_with_synonyms(keywords):
    out = []
    for kw in keywords:
        out.append(kw)
        out.extend(SYNONYM_MAP.get(kw, []))
    seen=set(); res=[]
    for k in out:
        if k not in seen:
            res.append(k); seen.add(k)
    return res

def exclude_by_title_keywords(title):
    for w in EXCLUDE_TITLE_KEYWORDS:
        if w in title:
            return True
    return False

def infer_source_from_url(url):
    domain = urlparse(url).netloc
    if domain.startswith("www."):
        domain = domain[4:]
    return domain

NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")


def fetch_naver_news(query, start_date=None, end_date=None, limit=300, require_keyword_in_title=False):
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        return []
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    articles=[]
    for start in range(1, 1001, 100):
        if len(articles) >= limit: break
        params={"query": query, "display":100, "start":start, "sort":"date"}
        try:
            r=requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params, timeout=20)
        except Exception:
            break
        if r.status_code != 200: break
        items = r.json().get("items", [])
        for it in items:
            title = html.unescape(re.sub("<.*?>","",it["title"]))
            desc  = html.unescape(re.sub("<.*?>","",it["description"]))
            try:
                pub_date = dt.datetime.strptime(it["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            except:
                continue
            if start_date and pub_date < start_date: continue
            if end_date and pub_date > end_date: continue
            if require_keyword_in_title and query.lower() not in title.lower(): continue
            if exclude_by_title_keywords(title): continue
            link = it.get("originallink") or it.get("link")
            source = infer_source_from_url(link) if link else "Naver"
            articles.append({
                "title": title, "description": desc, "link": link,
                "date": pub_date.strftime("%Y-%m-%d"), "source": source, "ê²€ìƒ‰ì–´": query
            })
        if len(items) < 100: break
    return articles[:limit]

def is_similar(a, b, th=0.5):
    return difflib.SequenceMatcher(None, a, b).ratio() >= th

def remove_duplicates(arts):
    uniq=[]; titles=[]
    for a in arts:
        t=a.get("title","" )
        if all(not is_similar(t, tt) for tt in titles):
            uniq.append(a); titles.append(t)
    return uniq

def get_alt_invest_credit_keywords():
    return CFG.get("sector_filter_categories", {})

from openai import OpenAI
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

def extract_article_text(url, fallback_desc=None):
    try:
        import newspaper
        art = newspaper.Article(url, language='ko')
        art.download(); art.parse()
        text = art.text.strip()
        if len(text) < 80:
            raise Exception("short")
        return text
    except Exception:
        return fallback_desc or ""

def summarize_article(url, title, description, target_keyword):
    if not client:
        return ("OpenAI API í‚¤ ë¯¸ì„¤ì •", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", description or "")
    full_text = extract_article_text(url, fallback_desc=description) or (title + "\n" + (description or ""))
    sector_kw_map = get_alt_invest_credit_keywords()
    prompt = f"""
[ì„¹í„°ë³„ ëŒ€ì²´íˆ¬ì í•µì‹¬ ì´ìŠˆ í‚¤ì›Œë“œ]
{{json.dumps(sector_kw_map, ensure_ascii=False, indent=2)}}

ì•„ë˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ ë¶„ì„í•´, '{target_keyword}'ì™€ ì§ì ‘ ê´€ë ¨ëœ **ëŒ€ì²´íˆ¬ì ê´€ì **ì—ì„œë§Œ ì‘ë‹µí•˜ì‹œì˜¤.

1. [ì‹¬ì¸µ ì‹œì‚¬ì ]: ì‹ ìš©í‰ê°€ ë¦¬í¬íŠ¸ í†¤ìœ¼ë¡œ ì‘ì„±. 
   - (ê±°ë˜/ì¡°ë‹¬) ë¦¬íŒŒì´ë‚¸ì‹±Â·ë§Œê¸°ë²½Â·Covenant(headroom/íŠ¸ë¦¬ê±°)Â·ë ˆë²„ë¦¬ì§€ ë³€í™”Â·ìœ ë™ì„± ì§€ì› ì—¬ë¶€
   - (ìš´ì˜/í˜„ê¸ˆíë¦„) DSCRÂ·ê°€ë™ë¥ /ì´ìš©ë¥ Â·ê°€ê²©(PPA/SMP/ì„ëŒ€ë£Œ)Â·CapEx/O&M ë³€í™”Â·ë¹„ìš© ë¯¼ê°ë„
   - (ê³„ì•½/ê·œì œ) ì¥ê¸°ê³„ì•½ ì•ˆì •ì„±(AnchorÂ·LTA), ì»¨ì„¸ì…˜/ì •ì±…/ê·œì œ ë³€í™”ì˜ ë“±ê¸‰ ì˜í–¥
   - (ë°©í–¥ì„±) ë“±ê¸‰/ì „ë§ì— ëŒ€í•œ ìƒí–¥Â·í•˜í–¥ ìš”ì¸ê³¼ ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸ë¥¼ 2~3ë¬¸ì¥ ì´ìƒìœ¼ë¡œ
2. [í•œ ì¤„ ì‹œì‚¬ì ]: ìœ„ ë‚´ìš©ì„ **íˆ¬ìì ì˜ì‚¬ê²°ì •** ê´€ì ì—ì„œ í•œ ë¬¸ì¥ìœ¼ë¡œ.
3. [í•œ ì¤„ ìš”ì•½]: ì£¼ì²´Â·í•µì‹¬ ì‚¬ê±´Â·ê²°ê³¼ë¥¼ ê°„ê²°íˆ.
4. [ê²€ìƒ‰ í‚¤ì›Œë“œ]: ê¸°ì‚¬ì— ì‚¬ìš©ëœ ê´€ë ¨ í‚¤ì›Œë“œ(ì½¤ë§ˆ êµ¬ë¶„).
5. [ê°ì„±]: ê¸ì •/ë¶€ì • ì¤‘ í•˜ë‚˜(ì‹ ìš©ë„ ì˜í–¥ ê¸°ì¤€).
6. [ì£¼ìš” í‚¤ì›Œë“œ]: ì¸ë¬¼Â·ê¸°ì—…Â·ì¡°ì§ëª…ë§Œ ì½¤ë§ˆë¡œ.

[ê¸°ì‚¬ ë³¸ë¬¸]
{full_text}
""".strip()

    try:
        res = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role":"system","content":"ë„ˆëŠ” ëŒ€ì²´íˆ¬ìÂ·ì‹ ìš©í‰ê°€ ë¶„ì„ê°€ë‹¤. ì •í™•Â·ê°„ê²°í•˜ê²Œ ë‹µí•˜ë¼."},
                      {"role":"user","content":prompt}],
            temperature=0, max_tokens=800
        )
        answer = res.choices[0].message.content.strip()
        def pick(tag):
            m = re.search(rf"\[{tag}\]:\s*([\s\S]+?)(?=\n\[\w+ ?\w*\]:|$)", answer)
            return m.group(1).strip() if m else ""
        senti_raw = pick("ê°ì„±")
        senti = "ê¸ì •" if senti_raw in ("ê¸ì •","positive","Positive") else ("ë¶€ì •" if senti_raw in ("ë¶€ì •","negative","Negative") else "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨")
        return (
            pick("í•œ ì¤„ ìš”ì•½") or "ìš”ì•½ ì‹¤íŒ¨",
            pick("ê²€ìƒ‰ í‚¤ì›Œë“œ"),
            senti,
            pick("ì‹¬ì¸µ ì‹œì‚¬ì ") or "ì‹œì‚¬ì  ì¶”ì¶œ ì‹¤íŒ¨",
            pick("í•œ ì¤„ ì‹œì‚¬ì ") or "",
            full_text
        )
    except Exception as e:
        return (f"ìš”ì•½ ì˜¤ë¥˜: {e}", "", "ê°ì„± ì¶”ì¶œ ì‹¤íŒ¨", "", "", full_text)

def run_search_by_keywords(keywords):
    expanded = expand_keywords_with_synonyms(keywords)
    results = []
    with ThreadPoolExecutor(max_workers=min(5, len(expanded) or 1)) as ex:
        futs = {ex.submit(fetch_naver_news, kw, st.session_state["start_date"], st.session_state["end_date"],
                          require_keyword_in_title=st.session_state["require_exact_keyword_in_title_or_content"]): kw for kw in expanded}
        for fu in as_completed(futs):
            kw = futs[fu]
            try:
                arts = fu.result()
                for a in arts:
                    a["í‚¤ì›Œë“œ"] = kw
                results.extend(arts)
            except Exception:
                pass
    if st.session_state["remove_duplicate_articles"]:
        results = remove_duplicates(results)
    for main in keywords:
        st.session_state["search_results"][main] = [a for a in results if a["ê²€ìƒ‰ì–´"] == main or a["í‚¤ì›Œë“œ"] == main]

def run_search_by_sectors(sectors):
    entities = []
    for s in sectors:
        entities += FAVORITES.get(s, [])
    if not entities: return
    run_search_by_keywords(list(dict.fromkeys(entities)))

keywords_input = [k.strip() for k in st.session_state["keyword_input"].split(",") if k.strip()]
if kw_btn and keywords_input:
    st.session_state["search_results"] = {}
    run_search_by_keywords(keywords_input)

if cat_btn and selected_categories:
    st.session_state["search_results"] = {}
    run_search_by_sectors(selected_categories)

ALL_COMMON = [k for v in COMMON_FILTERS.values() for k in v]

def text_contains_any(text, keywords):
    t = (text or "").lower()
    return any(k.lower() in t for k in keywords)

def article_passes_all_filters(article):
    if exclude_by_title_keywords(article.get("title","")): return False
    try:
        d = dt.datetime.strptime(article.get("date",""), "%Y-%m-%d").date()
        if d < st.session_state["start_date"] or d > st.session_state["end_date"]:
            return False
    except: 
        return False
    if st.session_state["filter_allowed_sources_only"]:
        src = (article.get("source","") or "").lower()
        if src.startswith("www."): src = src[4:]
        if src not in ALLOWED_SOURCES:
            return False
    if not text_contains_any((article.get("title","") + " " + article.get("description","")), ALL_COMMON):
        return False
    sector_passed = True
    if st.session_state["use_sector_filter"]:
        key = article.get("í‚¤ì›Œë“œ") or article.get("ê²€ìƒ‰ì–´")
        matched_sector=None
        for sec, ents in FAVORITES.items():
            if key in ents:
                matched_sector = sec; break
        if matched_sector:
            sub_filters = st.session_state.get("sector_filter_map", {}).get(matched_sector, [])
            if sub_filters:
                sector_passed = text_contains_any((article.get("title","") + " " + article.get("description","")), sub_filters)
    kw_pass = True
    if st.session_state["require_exact_keyword_in_title_or_content"]:
        keys=[]
        if keywords_input: keys += keywords_input
        for s in selected_categories: keys += FAVORITES.get(s, [])
        kw_pass = text_contains_any((article.get("title","")+ " " + article.get("description","")), keys)
    if not (sector_passed or kw_pass):
        return False
    return True

if st.session_state["search_results"]:
    st.markdown("### ğŸ” ê²€ìƒ‰ ê²°ê³¼")
    for bucket, articles in st.session_state["search_results"].items():
        st.markdown(f"#### â€¢ {bucket}")
        cnt = 0
        for a in articles:
            if not article_passes_all_filters(a): 
                continue
            cnt += 1
            with st.container():
                st.markdown(f"**<span class='news-title'>{a['title']}</span>**", unsafe_allow_html=True)
                st.caption(f"{a.get('date')} Â· {a.get('source','')} Â· í‚¤ì›Œë“œ: {a.get('í‚¤ì›Œë“œ') or a.get('ê²€ìƒ‰ì–´')}")
                st.write(a.get("description",""))
                if a.get("link"): st.markdown(f"[ì›ë¬¸ë³´ê¸°]({a.get('link')})")
                if st.session_state["enable_summary"]:
                    with st.spinner("ìš”ì•½ ì¤‘..."):
                        one_line, kw, senti, implication, short_imp, _ = summarize_article(a.get("link"), a.get("title"), a.get("description"), a.get("í‚¤ì›Œë“œ") or a.get("ê²€ìƒ‰ì–´"))
                    st.markdown(f"- **í•œ ì¤„ ìš”ì•½**: {one_line}")
                    st.markdown(f"- **í•œ ì¤„ ì‹œì‚¬ì **: {short_imp}")
                    st.markdown(f"- **ì‹¬ì¸µ ì‹œì‚¬ì **: {implication}")
                    st.markdown(f"- **ê°ì„±**: {senti}")
        if cnt == 0:
            st.info("í‘œì‹œí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. (í•„í„°ì— ëª¨ë‘ ê±¸ëŸ¬ì§)")
else:
    st.info("ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê²€ìƒ‰ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
