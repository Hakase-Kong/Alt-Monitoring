
# -*- coding: utf-8 -*-
import os, re, json, time, html, difflib
import datetime as dt
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

import streamlit as st
import pandas as pd
import requests

st.set_page_config(page_title="Alternative Investment Monitoring", layout="wide")

with open("config.json", "r", encoding="utf-8") as f:
    CFG = json.load(f)

EXCLUDE_TITLE_KEYWORDS = CFG["EXCLUDE_TITLE_KEYWORDS"]
ALLOWED_SOURCES = set(CFG["ALLOWED_SOURCES"])
FAVORITES = CFG["favorite_categories"]
SECTOR_FILTERS = CFG.get("sector_filter_categories", {})
COMMON_FILTERS = CFG["common_filter_categories"]
SYNONYM_MAP = CFG.get("synonym_map", {})

def init_state():
    defaults = {
        "start_date": dt.date.today() - dt.timedelta(days=7),
        "end_date": dt.date.today(),
        "keyword_input": "",
        "cat_multi": [],
        "search_results": {},
        "show_limit": {},
        "remove_duplicate_articles": True,
        "require_exact_keyword_in_title_or_content": True,
        "filter_allowed_sources_only": False,
        "use_sector_filter": True,
        "sector_filter_map": {},
        "show_sentiment_badge": False,
        "enable_summary": True,
    }
    for k,v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v
init_state()

col_title, col_option1, col_option2 = st.columns([0.6, 0.2, 0.2])
with col_title:
    st.markdown(
        "<h1 style='color:#1a1a1a; margin-bottom:0.5rem;'>üíº Alternative Investment Monitoring</h1>",
        unsafe_allow_html=True
    )
with col_option1:
    st.checkbox("Í∞êÏÑ±Î∂ÑÏÑù Î∞∞ÏßÄÌëúÏãú", key="show_sentiment_badge")
with col_option2:
    st.checkbox("ÏöîÏïΩ Í∏∞Îä•", key="enable_summary")

st.markdown("""
<style>
[data-testid='column'] > div { gap: 0rem !important; }
.stBox { background: #fcfcfc; border-radius: 0.7em; border: 1.5px solid #e0e2e6; margin-bottom: 1.2em; padding: 1.1em; box-shadow: 0 2px 8px rgba(0,0,0,0.03); }
.news-title { word-break: break-all !important; white-space: normal !important; display:block; overflow:visible; }
</style>
""", unsafe_allow_html=True)

col_kw_input, col_kw_btn = st.columns([0.8, 0.2])
with col_kw_input:
    st.text_input(label="", value="", key="keyword_input", placeholder="ÏâºÌëú(,)Î°ú Îã§Ï§ë ÌÇ§ÏõåÎìú ÏûÖÎ†•", label_visibility="collapsed")
with col_kw_btn:
    kw_btn = st.button("Í≤ÄÏÉâ", use_container_width=True)

st.markdown("**‚≠ê ÏÑπÌÑ∞ ÏÑ†ÌÉù**")
col_cat_input, col_cat_btn = st.columns([0.8, 0.2])
with col_cat_input:
    selected_categories = st.multiselect(
        label="",
        options=list(FAVORITES.keys()),
        key="cat_multi",
        label_visibility="collapsed"
    )
with col_cat_btn:
    cat_btn = st.button("üîç Í≤ÄÏÉâ", use_container_width=True)

date_col1, date_col2 = st.columns([1,1])
with date_col1:
    st.date_input("ÏãúÏûëÏùº", key="start_date")
with date_col2:
    st.date_input("Ï¢ÖÎ£åÏùº", key="end_date")

with st.expander("üß© Í≥µÌÜµ ÌïÑÌÑ∞ ÏòµÏÖò (Ìï≠ÏÉÅ Ï†ÅÏö©Îê®)"):
    for major, subs in COMMON_FILTERS.items():
        st.markdown(f"**{major}**: {', '.join(subs)}" )

with st.expander("üìä ÏÑπÌÑ∞Î≥Ñ ÌïÑÌÑ∞ ÏòµÏÖò (ÎåÄÎ∂ÑÎ•òÎ≥Ñ ÏÑ∏Î∂Ä Ïù¥Ïäà ÌïÑÌÑ∞ÎßÅ)"):
    st.checkbox("Ïù¥ ÌïÑÌÑ∞ Ï†ÅÏö©", key="use_sector_filter")
    updated_map = {}
    for sector in selected_categories:
        options = SECTOR_FILTERS.get(sector, [])
        default_selected = st.session_state.get("sector_filter_map", {}).get(sector, options)
        selected_sub = st.multiselect(
            f"{sector} ÏÑ∏Î∂Ä Ïù¥Ïäà ÌÇ§ÏõåÎìú",
            options=options,
            default=default_selected,
            key=f"subfilter_sector_{sector}"
        )
        updated_map[sector] = selected_sub
    st.session_state["sector_filter_map"] = updated_map

with st.expander("üîç ÌÇ§ÏõåÎìú ÌïÑÌÑ∞ ÏòµÏÖò"):
    st.checkbox("ÌÇ§ÏõåÎìúÍ∞Ä Ï†úÎ™© ÎòêÎäî Î≥∏Î¨∏Ïóê Ìè¨Ìï®Îêú Í∏∞ÏÇ¨Îßå Î≥¥Í∏∞", key="require_exact_keyword_in_title_or_content")
    st.checkbox("Ï§ëÎ≥µ Í∏∞ÏÇ¨ Ï†úÍ±∞", key="remove_duplicate_articles")
    st.checkbox("ÌäπÏ†ï Ïñ∏Î°†ÏÇ¨Îßå Í≤ÄÏÉâ", key="filter_allowed_sources_only", help="ALLOWED_SOURCES Î™©Î°ùÎßå ÌóàÏö©")


def expand_keywords_with_synonyms(keywords):
    out = []
    for kw in keywords:
        out.append(kw)
        out.extend(SYNONYM_MAP.get(kw, []))
    seen=set(); res=[]
    for k in out:
        if k not in seen:
            res.append(k); seen.add(k)
    return res

def exclude_by_title_keywords(title):
    for w in EXCLUDE_TITLE_KEYWORDS:
        if w in title:
            return True
    return False

def infer_source_from_url(url):
    domain = urlparse(url).netloc
    if domain.startswith("www."):
        domain = domain[4:]
    return domain

NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")


def fetch_naver_news(query, start_date=None, end_date=None, limit=300, require_keyword_in_title=False):
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        return []
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    articles=[]
    for start in range(1, 1001, 100):
        if len(articles) >= limit: break
        params={"query": query, "display":100, "start":start, "sort":"date"}
        try:
            r=requests.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params, timeout=20)
        except Exception:
            break
        if r.status_code != 200: break
        items = r.json().get("items", [])
        for it in items:
            title = html.unescape(re.sub("<.*?>","",it["title"]))
            desc  = html.unescape(re.sub("<.*?>","",it["description"]))
            try:
                pub_date = dt.datetime.strptime(it["pubDate"], "%a, %d %b %Y %H:%M:%S %z").date()
            except:
                continue
            if start_date and pub_date < start_date: continue
            if end_date and pub_date > end_date: continue
            if require_keyword_in_title and query.lower() not in title.lower(): continue
            if exclude_by_title_keywords(title): continue
            link = it.get("originallink") or it.get("link")
            source = infer_source_from_url(link) if link else "Naver"
            articles.append({
                "title": title, "description": desc, "link": link,
                "date": pub_date.strftime("%Y-%m-%d"), "source": source, "Í≤ÄÏÉâÏñ¥": query
            })
        if len(items) < 100: break
    return articles[:limit]

def is_similar(a, b, th=0.5):
    return difflib.SequenceMatcher(None, a, b).ratio() >= th

def remove_duplicates(arts):
    uniq=[]; titles=[]
    for a in arts:
        t=a.get("title","" )
        if all(not is_similar(t, tt) for tt in titles):
            uniq.append(a); titles.append(t)
    return uniq

def get_alt_invest_credit_keywords():
    return CFG.get("sector_filter_categories", {})

from openai import OpenAI
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

def extract_article_text(url, fallback_desc=None):
    try:
        import newspaper
        art = newspaper.Article(url, language='ko')
        art.download(); art.parse()
        text = art.text.strip()
        if len(text) < 80:
            raise Exception("short")
        return text
    except Exception:
        return fallback_desc or ""

def summarize_article(url, title, description, target_keyword):
    if not client:
        return ("OpenAI API ÌÇ§ ÎØ∏ÏÑ§Ï†ï", "", "Í∞êÏÑ± Ï∂îÏ∂ú Ïã§Ìå®", "", "", description or "")
    full_text = extract_article_text(url, fallback_desc=description) or (title + "\n" + (description or ""))
    sector_kw_map = get_alt_invest_credit_keywords()
    prompt = f"""
[ÏÑπÌÑ∞Î≥Ñ ÎåÄÏ≤¥Ìà¨Ïûê ÌïµÏã¨ Ïù¥Ïäà ÌÇ§ÏõåÎìú]
{{json.dumps(sector_kw_map, ensure_ascii=False, indent=2)}}

ÏïÑÎûò Í∏∞ÏÇ¨ Î≥∏Î¨∏ÏùÑ Î∂ÑÏÑùÌï¥, '{target_keyword}'ÏôÄ ÏßÅÏ†ë Í¥ÄÎ†®Îêú **ÎåÄÏ≤¥Ìà¨Ïûê Í¥ÄÏ†ê**ÏóêÏÑúÎßå ÏùëÎãµÌïòÏãúÏò§.

1. [Ïã¨Ï∏µ ÏãúÏÇ¨Ï†ê]: Ïã†Ïö©ÌèâÍ∞Ä Î¶¨Ìè¨Ìä∏ ÌÜ§ÏúºÎ°ú ÏûëÏÑ±. 
   - (Í±∞Îûò/Ï°∞Îã¨) Î¶¨ÌååÏù¥ÎÇ∏Ïã±¬∑ÎßåÍ∏∞Î≤Ω¬∑Covenant(headroom/Ìä∏Î¶¨Í±∞)¬∑Î†àÎ≤ÑÎ¶¨ÏßÄ Î≥ÄÌôî¬∑Ïú†ÎèôÏÑ± ÏßÄÏõê Ïó¨Î∂Ä
   - (Ïö¥ÏòÅ/ÌòÑÍ∏àÌùêÎ¶Ñ) DSCR¬∑Í∞ÄÎèôÎ•†/Ïù¥Ïö©Î•†¬∑Í∞ÄÍ≤©(PPA/SMP/ÏûÑÎåÄÎ£å)¬∑CapEx/O&M Î≥ÄÌôî¬∑ÎπÑÏö© ÎØºÍ∞êÎèÑ
   - (Í≥ÑÏïΩ/Í∑úÏ†ú) Ïû•Í∏∞Í≥ÑÏïΩ ÏïàÏ†ïÏÑ±(Anchor¬∑LTA), Ïª®ÏÑ∏ÏÖò/Ï†ïÏ±Ö/Í∑úÏ†ú Î≥ÄÌôîÏùò Îì±Í∏â ÏòÅÌñ•
   - (Î∞©Ìñ•ÏÑ±) Îì±Í∏â/Ï†ÑÎßùÏóê ÎåÄÌïú ÏÉÅÌñ•¬∑ÌïòÌñ• ÏöîÏù∏Í≥º Î™®ÎãàÌÑ∞ÎßÅ Ìè¨Ïù∏Ìä∏Î•º 2~3Î¨∏Ïû• Ïù¥ÏÉÅÏúºÎ°ú
2. [Ìïú Ï§Ñ ÏãúÏÇ¨Ï†ê]: ÏúÑ ÎÇ¥Ïö©ÏùÑ **Ìà¨ÏûêÏûê ÏùòÏÇ¨Í≤∞Ï†ï** Í¥ÄÏ†êÏóêÏÑú Ìïú Î¨∏Ïû•ÏúºÎ°ú.
3. [Ìïú Ï§Ñ ÏöîÏïΩ]: Ï£ºÏ≤¥¬∑ÌïµÏã¨ ÏÇ¨Í±¥¬∑Í≤∞Í≥ºÎ•º Í∞ÑÍ≤∞Ìûà.
4. [Í≤ÄÏÉâ ÌÇ§ÏõåÎìú]: Í∏∞ÏÇ¨Ïóê ÏÇ¨Ïö©Îêú Í¥ÄÎ†® ÌÇ§ÏõåÎìú(ÏΩ§Îßà Íµ¨Î∂Ñ).
5. [Í∞êÏÑ±]: Í∏çÏ†ï/Î∂ÄÏ†ï Ï§ë ÌïòÎÇò(Ïã†Ïö©ÎèÑ ÏòÅÌñ• Í∏∞Ï§Ä).
6. [Ï£ºÏöî ÌÇ§ÏõåÎìú]: Ïù∏Î¨º¬∑Í∏∞ÏóÖ¬∑Ï°∞ÏßÅÎ™ÖÎßå ÏΩ§ÎßàÎ°ú.

[Í∏∞ÏÇ¨ Î≥∏Î¨∏]
{full_text}
""".strip()

    try:
        res = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role":"system","content":"ÎÑàÎäî ÎåÄÏ≤¥Ìà¨Ïûê¬∑Ïã†Ïö©ÌèâÍ∞Ä Î∂ÑÏÑùÍ∞ÄÎã§. Ï†ïÌôï¬∑Í∞ÑÍ≤∞ÌïòÍ≤å ÎãµÌïòÎùº."},
                      {"role":"user","content":prompt}],
            temperature=0, max_tokens=800
        )
        answer = res.choices[0].message.content.strip()
        def pick(tag):
            m = re.search(rf"\[{tag}\]:\s*([\s\S]+?)(?=\n\[\w+ ?\w*\]:|$)", answer)
            return m.group(1).strip() if m else ""
        senti_raw = pick("Í∞êÏÑ±")
        senti = "Í∏çÏ†ï" if senti_raw in ("Í∏çÏ†ï","positive","Positive") else ("Î∂ÄÏ†ï" if senti_raw in ("Î∂ÄÏ†ï","negative","Negative") else "Í∞êÏÑ± Ï∂îÏ∂ú Ïã§Ìå®")
        return (
            pick("Ìïú Ï§Ñ ÏöîÏïΩ") or "ÏöîÏïΩ Ïã§Ìå®",
            pick("Í≤ÄÏÉâ ÌÇ§ÏõåÎìú"),
            senti,
            pick("Ïã¨Ï∏µ ÏãúÏÇ¨Ï†ê") or "ÏãúÏÇ¨Ï†ê Ï∂îÏ∂ú Ïã§Ìå®",
            pick("Ìïú Ï§Ñ ÏãúÏÇ¨Ï†ê") or "",
            full_text
        )
    except Exception as e:
        return (f"ÏöîÏïΩ Ïò§Î•ò: {e}", "", "Í∞êÏÑ± Ï∂îÏ∂ú Ïã§Ìå®", "", "", full_text)

def run_search_by_keywords(keywords):
    expanded = expand_keywords_with_synonyms(keywords)
    results = []
    with ThreadPoolExecutor(max_workers=min(5, len(expanded) or 1)) as ex:
        futs = {ex.submit(fetch_naver_news, kw, st.session_state["start_date"], st.session_state["end_date"],
                          require_keyword_in_title=st.session_state["require_exact_keyword_in_title_or_content"]): kw for kw in expanded}
        for fu in as_completed(futs):
            kw = futs[fu]
            try:
                arts = fu.result()
                for a in arts:
                    a["ÌÇ§ÏõåÎìú"] = kw
                results.extend(arts)
            except Exception:
                pass
    if st.session_state["remove_duplicate_articles"]:
        results = remove_duplicates(results)
    for main in keywords:
        st.session_state["search_results"][main] = [a for a in results if a["Í≤ÄÏÉâÏñ¥"] == main or a["ÌÇ§ÏõåÎìú"] == main]

def run_search_by_sectors(sectors):
    entities = []
    for s in sectors:
        entities += FAVORITES.get(s, [])
    if not entities: return
    run_search_by_keywords(list(dict.fromkeys(entities)))

keywords_input = [k.strip() for k in st.session_state["keyword_input"].split(",") if k.strip()]
if kw_btn and keywords_input:
    st.session_state["search_results"] = {}
    run_search_by_keywords(keywords_input)

if cat_btn and selected_categories:
    st.session_state["search_results"] = {}
    run_search_by_sectors(selected_categories)

ALL_COMMON = [k for v in COMMON_FILTERS.values() for k in v]

def text_contains_any(text, keywords):
    t = (text or "").lower()
    return any(k.lower() in t for k in keywords)

def article_passes_all_filters(article):
    if exclude_by_title_keywords(article.get("title","")): return False
    try:
        d = dt.datetime.strptime(article.get("date",""), "%Y-%m-%d").date()
        if d < st.session_state["start_date"] or d > st.session_state["end_date"]:
            return False
    except: 
        return False
    if st.session_state["filter_allowed_sources_only"]:
        src = (article.get("source","") or "").lower()
        if src.startswith("www."): src = src[4:]
        if src not in ALLOWED_SOURCES:
            return False
    if not text_contains_any((article.get("title","") + " " + article.get("description","")), ALL_COMMON):
        return False
    sector_passed = True
    if st.session_state["use_sector_filter"]:
        key = article.get("ÌÇ§ÏõåÎìú") or article.get("Í≤ÄÏÉâÏñ¥")
        matched_sector=None
        for sec, ents in FAVORITES.items():
            if key in ents:
                matched_sector = sec; break
        if matched_sector:
            sub_filters = st.session_state.get("sector_filter_map", {}).get(matched_sector, [])
            if sub_filters:
                sector_passed = text_contains_any((article.get("title","") + " " + article.get("description","")), sub_filters)
    kw_pass = True
    if st.session_state["require_exact_keyword_in_title_or_content"]:
        keys=[]
        if keywords_input: keys += keywords_input
        for s in selected_categories: keys += FAVORITES.get(s, [])
        kw_pass = text_contains_any((article.get("title","")+ " " + article.get("description","")), keys)
    if not (sector_passed or kw_pass):
        return False
    return True

if st.session_state["search_results"]:
    st.markdown("### üîé Í≤ÄÏÉâ Í≤∞Í≥º")
    for bucket, articles in st.session_state["search_results"].items():
        st.markdown(f"#### ‚Ä¢ {bucket}")
        cnt = 0
        for a in articles:
            if not article_passes_all_filters(a): 
                continue
            cnt += 1
            with st.container():
                st.markdown(f"**<span class='news-title'>{a['title']}</span>**", unsafe_allow_html=True)
                st.caption(f"{a.get('date')} ¬∑ {a.get('source','')} ¬∑ ÌÇ§ÏõåÎìú: {a.get('ÌÇ§ÏõåÎìú') or a.get('Í≤ÄÏÉâÏñ¥')}")
                st.write(a.get("description",""))
                if a.get("link"): st.markdown(f"[ÏõêÎ¨∏Î≥¥Í∏∞]({a.get('link')})")
                if st.session_state["enable_summary"]:
                    with st.spinner("ÏöîÏïΩ Ï§ë..."):
                        one_line, kw, senti, implication, short_imp, _ = summarize_article(a.get("link"), a.get("title"), a.get("description"), a.get("ÌÇ§ÏõåÎìú") or a.get("Í≤ÄÏÉâÏñ¥"))
                    st.markdown(f"- **Ìïú Ï§Ñ ÏöîÏïΩ**: {one_line}")
                    st.markdown(f"- **Ìïú Ï§Ñ ÏãúÏÇ¨Ï†ê**: {short_imp}")
                    st.markdown(f"- **Ïã¨Ï∏µ ÏãúÏÇ¨Ï†ê**: {implication}")
                    st.markdown(f"- **Í∞êÏÑ±**: {senti}")
        if cnt == 0:
            st.info("ÌëúÏãúÌï† Í∏∞ÏÇ¨Í∞Ä ÏóÜÏäµÎãàÎã§. (ÌïÑÌÑ∞Ïóê Î™®Îëê Í±∏Îü¨Ïßê)")
else:
    st.info("Îâ¥Ïä§ Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä Í≤ÄÏÉâÏùÑ Ïã§ÌñâÌï¥ Ï£ºÏÑ∏Ïöî.")
